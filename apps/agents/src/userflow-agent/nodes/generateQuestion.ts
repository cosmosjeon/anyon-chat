import { AIMessage } from "@langchain/core/messages";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { UserFlowReturnType, UserFlowState } from "../state";
import { DynamicQuestion } from "../types";
import { getModelFromConfig } from "../../utils";
import { GENERATE_QUESTION_PROMPT } from "../prompts";

/**
 * Generate Question Node
 *
 * Dynamically generates the next question based on:
 * - Current user flow completeness
 * - Previous answers and context
 * - Question stage (1-8)
 */
export async function generateQuestion(
  state: UserFlowState,
  config: LangGraphRunnableConfig
): Promise<UserFlowReturnType> {
  console.log("[generateQuestion] entering:", {
    currentQuestionCount: state.currentQuestionCount,
    maxQuestions: state.maxQuestions,
    awaitingAnswer: state.awaitingAnswer,
    needsFollowup: state.needsFollowup,
  });

  const {
    userFlowContext,
    answers,
    currentQuestionCount,
    maxQuestions,
    completenessScore,
    needsFollowup,
    customQuestionText,
  } = state;

  // Check if we should finish (reached max questions or high completeness)
  if (currentQuestionCount >= maxQuestions || completenessScore >= 95) {
    return {
      isComplete: true,
    };
  }

  // Determine current stage based on question count
  const stage = determineQuestionStage(currentQuestionCount);

  // Build context for AI
  const contextText = buildContextText(userFlowContext, answers);

  // Use custom question if provided (for follow-ups)
  if (needsFollowup && customQuestionText) {
    const questionMessage = new AIMessage({
      content: `ğŸ’¡ **ê¼¬ë¦¬ ì§ˆë¬¸** (${currentQuestionCount + 1}/${maxQuestions})

${customQuestionText}`,
    });

    return {
      messages: [questionMessage],
      awaitingAnswer: true,
      currentQuestionCount: currentQuestionCount + 1,
      needsFollowup: false,
      customQuestionText: undefined,
    };
  }

  // Generate dynamic question using AI
  const model = await getModelFromConfig(config);

  const prompt = GENERATE_QUESTION_PROMPT.replace("{context}", contextText).replace(
    "{questionStage}",
    stage.description
  );

  try {
    const response = await model.invoke([
      { role: "system", content: "ë‹¹ì‹ ì€ UX ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì í”Œë¡œìš°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤." },
      { role: "user", content: prompt },
    ]);

    const responseText = response.content.toString();

    // Try to parse JSON response
    let questionData: any;
    try {
      // Extract JSON from response (might be wrapped in markdown)
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        questionData = JSON.parse(jsonMatch[0]);
      } else {
        // Fallback: use the response as-is
        questionData = {
          questionText: responseText,
          choices: [],
          isFollowUp: false,
          stage: stage.name,
        };
      }
    } catch (parseError) {
      // If parsing fails, use response text directly
      questionData = {
        questionText: responseText,
        choices: [],
        isFollowUp: false,
        stage: stage.name,
      };
    }

    // Format the question message
    const questionText = formatQuestionMessage(
      questionData,
      currentQuestionCount + 1,
      maxQuestions,
      stage.name
    );

    const questionMessage = new AIMessage({
      content: questionText,
      additional_kwargs: {
        dynamicQuestion: questionData,
      },
    });

    // Create DynamicQuestion object
    const dynamicQuestion: DynamicQuestion = {
      id: `uf_q${currentQuestionCount + 1}`,
      questionText: questionData.questionText,
      choices: questionData.choices || [],
      context: stage.name,
      isFollowUp: questionData.isFollowUp || false,
    };

    return {
      messages: [questionMessage],
      awaitingAnswer: true,
      currentQuestionCount: currentQuestionCount + 1,
      latestDynamicQuestion: dynamicQuestion,
    };
  } catch (error) {
    console.error("[generateQuestion] error generating question:", error);

    // Fallback question
    const fallbackMessage = new AIMessage({
      content: `ì§ˆë¬¸ ${currentQuestionCount + 1}/${maxQuestions}

${stage.name}ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
    });

    return {
      messages: [fallbackMessage],
      awaitingAnswer: true,
      currentQuestionCount: currentQuestionCount + 1,
    };
  }
}

/**
 * Determine question stage based on count
 * Based on AI_USERFLOW_QA_EXAMPLE.md 8-stage structure
 */
function determineQuestionStage(questionCount: number): {
  name: string;
  description: string;
} {
  if (questionCount < 2) {
    return {
      name: "1ë‹¨ê³„: ì „ì²´ í™”ë©´ êµ¬ì¡°",
      description: "í™”ë©´ ê°œìˆ˜ì™€ ê° í™”ë©´ì˜ ì´ë¦„/ì—­í• ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 5) {
    return {
      name: "2ë‹¨ê³„: ì²« ì‹¤í–‰ í”Œë¡œìš°",
      description: "ìŠ¤í”Œë˜ì‹œ, ë¡œê·¸ì¸, ì˜¨ë³´ë”© ë“± ì²« ì‹¤í–‰ ì‹œ ê²½í—˜ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 8) {
    return {
      name: "3ë‹¨ê³„: ë©”ì¸ í™”ë©´ êµ¬ì„±",
      description: "ë©”ì¸ í™”ë©´ì˜ ë ˆì´ì•„ì›ƒ, ì£¼ìš” ë²„íŠ¼, UI ìš”ì†Œë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 11) {
    return {
      name: "4ë‹¨ê³„: ì£¼ìš” ê¸°ëŠ¥ í™”ë©´",
      description: "í•µì‹¬ ê¸°ëŠ¥ í™”ë©´(ì¶”ê°€/í¸ì§‘/ìƒì„¸ ë“±)ì˜ êµ¬ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 13) {
    return {
      name: "5ë‹¨ê³„: ëª©ë¡ ìƒí˜¸ì‘ìš©",
      description: "ë¦¬ìŠ¤íŠ¸ í•­ëª© í´ë¦­, ì²´í¬ë°•ìŠ¤, ìŠ¤ì™€ì´í”„ ë“± ìƒí˜¸ì‘ìš©ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 16) {
    return {
      name: "6ë‹¨ê³„: ë¶€ê°€ ê¸°ëŠ¥ í™”ë©´",
      description: "ì„¤ì •, í†µê³„, í”„ë¡œí•„ ë“± ë¶€ê°€ í™”ë©´ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else if (questionCount < 19) {
    return {
      name: "7ë‹¨ê³„: ìœ ë£Œ ì „í™˜ í”Œë¡œìš°",
      description: "Freemiumì¸ ê²½ìš° ê²°ì œ í™”ë©´ê³¼ ìœ ë£Œ ì „í™˜ íë¦„ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
    };
  } else {
    return {
      name: "8ë‹¨ê³„: ì „ì²´ íë¦„ ì •ë¦¬",
      description: "ì „ì²´ ì‚¬ìš©ì ì—¬ì •ì„ í™•ì¸í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤.",
    };
  }
}

/**
 * Build context text from collected answers
 */
function buildContextText(
  context: any,
  answers: Array<{ questionText: string; answer: string }> | undefined
): string {
  const lines: string[] = [];

  // Add context info
  if (context.totalScreens) {
    lines.push(`í™”ë©´ ê°œìˆ˜: ${context.totalScreens}ê°œ`);
  }

  if (context.screenList && context.screenList.length > 0) {
    lines.push(
      `í™”ë©´ ëª©ë¡: ${context.screenList.map((s: any) => s.name).join(", ")}`
    );
  }

  if (context.mainScreenLayout) {
    lines.push(`ë©”ì¸ í™”ë©´ ë ˆì´ì•„ì›ƒ: ${context.mainScreenLayout}`);
  }

  if (context.loginMethod) {
    lines.push(`ë¡œê·¸ì¸ ë°©ì‹: ${context.loginMethod}`);
  }

  // Add recent answers (last 3)
  if (answers && answers.length > 0) {
    const recentAnswers = answers.slice(-3);
    lines.push("\nìµœê·¼ ë‹µë³€:");
    recentAnswers.forEach((a) => {
      lines.push(`Q: ${a.questionText}`);
      lines.push(`A: ${a.answer}`);
    });
  }

  return lines.length > 0 ? lines.join("\n") : "ì•„ì§ ìˆ˜ì§‘ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.";
}

/**
 * Format question message with choices
 */
function formatQuestionMessage(
  questionData: any,
  currentQuestion: number,
  maxQuestions: number,
  stageName: string
): string {
  const parts: string[] = [];

  // Progress header
  if (questionData.isFollowUp) {
    parts.push(`ğŸ’¡ **ê¼¬ë¦¬ ì§ˆë¬¸** (${currentQuestion}/${maxQuestions})\n`);
  } else {
    parts.push(`**ì§ˆë¬¸ ${currentQuestion}/${maxQuestions}** - ${stageName}\n`);
  }

  // Question text
  parts.push(questionData.questionText);

  // Choices (if provided)
  if (questionData.choices && questionData.choices.length > 0) {
    parts.push("\n");
    questionData.choices.forEach((choice: string) => {
      parts.push(choice);
    });
    parts.push("\nD) ì§ì ‘ ì…ë ¥");
  }

  return parts.join("\n");
}
